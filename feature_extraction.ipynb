{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>source</th>\n",
       "      <th>RDizzl3_seven</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>sentence_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Electoral College makes it so that candida...</td>\n",
       "      <td>0</td>\n",
       "      <td>Does the electoral college work?</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>True</td>\n",
       "      <td>417</td>\n",
       "      <td>2466</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The positive I think about driverless cars it ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Driverless cars</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>True</td>\n",
       "      <td>190</td>\n",
       "      <td>1001</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A face on mars sounds kind of creepy right? Ho...</td>\n",
       "      <td>0</td>\n",
       "      <td>The Face on Mars</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>True</td>\n",
       "      <td>166</td>\n",
       "      <td>880</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In this digital age, libraries have been a top...</td>\n",
       "      <td>1</td>\n",
       "      <td>Distance learning</td>\n",
       "      <td>falcon_180b_v1</td>\n",
       "      <td>False</td>\n",
       "      <td>317</td>\n",
       "      <td>2085</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I think that Lukes point of viewIf you were go...</td>\n",
       "      <td>0</td>\n",
       "      <td>\"A Cowboy Who Rode the Waves\"</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>True</td>\n",
       "      <td>161</td>\n",
       "      <td>771</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  The Electoral College makes it so that candida...      0   \n",
       "1  The positive I think about driverless cars it ...      0   \n",
       "2  A face on mars sounds kind of creepy right? Ho...      0   \n",
       "3  In this digital age, libraries have been a top...      1   \n",
       "4  I think that Lukes point of viewIf you were go...      0   \n",
       "\n",
       "                        prompt_name           source  RDizzl3_seven  \\\n",
       "0  Does the electoral college work?  persuade_corpus           True   \n",
       "1                   Driverless cars  persuade_corpus           True   \n",
       "2                  The Face on Mars  persuade_corpus           True   \n",
       "3                 Distance learning   falcon_180b_v1          False   \n",
       "4     \"A Cowboy Who Rode the Waves\"  persuade_corpus           True   \n",
       "\n",
       "   word_count  char_count  sentence_count  \n",
       "0         417        2466              20  \n",
       "1         190        1001               9  \n",
       "2         166         880               9  \n",
       "3         317        2085              19  \n",
       "4         161         771               9  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the balanced dataset\n",
    "data_balanced = pd.read_csv('balanced_dataset.csv')\n",
    "data_balanced.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joshuaphilip/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in human-written text: [('students', 55759), ('would', 48539), ('people', 41462), ('school', 27249), ('could', 26929), ('get', 26310), ('like', 22552), ('one', 22019), ('help', 21930), ('also', 21016), ('make', 20329), ('many', 20134), ('think', 19650), ('car', 18988), ('cars', 17407), ('time', 17180), ('even', 16023), ('student', 15774), ('electoral', 15273), ('good', 14507)]\n",
      "Most common words in AI-generated text: [('students', 33560), ('also', 23256), ('help', 21448), ('may', 19859), ('people', 18989), ('electoral', 18620), ('car', 17415), ('make', 17384), ('like,', 16090), ('important', 15424), ('college', 15303), ('school', 14217), ('would', 14021), ('one', 13455), ('could', 13270), ('time', 12808), ('limiting', 10892), ('lead', 10313), ('believe', 10002), ('usage', 9955)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize and count common words for each class\n",
    "def get_common_words(texts, n=20):\n",
    "    words = [word.lower() for text in texts for word in text.split() if word.lower() not in stop_words]\n",
    "    return Counter(words).most_common(n)\n",
    "\n",
    "# Common words in human-written text\n",
    "human_texts = data_balanced[data_balanced['label'] == 0]['text']\n",
    "common_human_words = get_common_words(human_texts)\n",
    "print(\"Most common words in human-written text:\", common_human_words)\n",
    "\n",
    "# Common words in AI-generated text\n",
    "ai_texts = data_balanced[data_balanced['label'] == 1]['text']\n",
    "common_ai_words = get_common_words(ai_texts)\n",
    "print(\"Most common words in AI-generated text:\", common_ai_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 n-grams: [('electoral college', 3973), ('car usage', 2708), ('limiting car', 1957), ('popular vote', 1515), ('cell phones', 1133), ('driverless cars', 983), ('community service', 951), ('united states', 945), ('extracurricular activities', 845), ('high school', 740), ('distance learning', 673), ('cell phone', 646), ('public transportation', 603), ('air pollution', 599), ('summer projects', 595), ('help students', 591), ('traffic congestion', 589), ('online classes', 549), ('greenhouse gas', 528), ('electoral votes', 517)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample a subset to avoid memory issues\n",
    "sample_data = data_balanced['text'].sample(5000, random_state=42)  # Adjust if needed\n",
    "\n",
    "# Define the vectorizer with reduced n-gram range and max features\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), max_features=500, stop_words='english')  # Bigrams only, limited to 500 features\n",
    "\n",
    "# Fit and transform the sample data\n",
    "X = vectorizer.fit_transform(sample_data)\n",
    "\n",
    "# Get top n-grams with counts\n",
    "ngram_counts = X.sum(axis=0).A1  # .A1 converts sparse matrix to array\n",
    "ngram_features = vectorizer.get_feature_names()  # Use get_feature_names() for older versions\n",
    "top_ngrams = sorted(zip(ngram_features, ngram_counts), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "# Display the top 20 n-grams\n",
    "print(\"Top 20 n-grams:\", top_ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count by Class:\n",
      "         count        mean         std    min    25%    50%    75%     max\n",
      "label                                                                     \n",
      "0      17497.0  416.402469  188.217259  143.0  272.0  382.0  518.0  1656.0\n",
      "1      17497.0  329.398983   94.256529    4.0  274.0  328.0  386.0   818.0\n",
      "Sentence Count by Class:\n",
      "         count       mean        std  min   25%   50%   75%    max\n",
      "label                                                             \n",
      "0      17497.0  20.593359  10.113202  0.0  13.0  19.0  26.0  216.0\n",
      "1      17497.0  17.198891   6.097075  0.0  13.0  17.0  20.0  122.0\n"
     ]
    }
   ],
   "source": [
    "# Word count, sentence length features were calculated in the previous notebook. Display basic stats by class.\n",
    "print(\"Word Count by Class:\")\n",
    "print(data_balanced.groupby('label')['word_count'].describe())\n",
    "\n",
    "print(\"Sentence Count by Class:\")\n",
    "print(data_balanced.groupby('label')['sentence_count'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average punctuation usage by class:\n",
      "label   \n",
      "0      !    0.274047\n",
      "       \"    2.730754\n",
      "       #    0.007944\n",
      "       $    0.042122\n",
      "       %    0.115105\n",
      "              ...   \n",
      "1      `    0.000686\n",
      "       {    0.000572\n",
      "       |    0.000000\n",
      "       }    0.000572\n",
      "       ~    0.000114\n",
      "Name: punctuation_counts, Length: 64, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Define a function to count punctuation\n",
    "import string\n",
    "\n",
    "def count_punctuation(text):\n",
    "    return {p: text.count(p) for p in string.punctuation}\n",
    "\n",
    "# Apply to each text entry\n",
    "data_balanced['punctuation_counts'] = data_balanced['text'].apply(count_punctuation)\n",
    "\n",
    "# Summarize average punctuation usage by class\n",
    "punctuation_summary = data_balanced.groupby('label')['punctuation_counts'].apply(lambda x: pd.DataFrame(x.tolist()).mean())\n",
    "print(\"Average punctuation usage by class:\")\n",
    "print(punctuation_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average POS counts by class:\n",
      "            NOUN       VERB        ADJ        ADV\n",
      "label                                            \n",
      "0      99.620278  83.532263  30.493056  24.928673\n",
      "1      88.519118  61.385037  31.467909  16.397897\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data for demonstration, using the balanced dataset\n",
    "data_balanced = pd.read_csv('balanced_dataset.csv')  # Load the balanced dataset\n",
    "\n",
    "# Define a function to extract POS counts\n",
    "def get_pos_counts(text):\n",
    "    blob = TextBlob(text)\n",
    "    pos_counts = {\"NOUN\": 0, \"VERB\": 0, \"ADJ\": 0, \"ADV\": 0}\n",
    "    for word, pos in blob.tags:\n",
    "        if pos.startswith(\"NN\"):  # Nouns\n",
    "            pos_counts[\"NOUN\"] += 1\n",
    "        elif pos.startswith(\"VB\"):  # Verbs\n",
    "            pos_counts[\"VERB\"] += 1\n",
    "        elif pos.startswith(\"JJ\"):  # Adjectives\n",
    "            pos_counts[\"ADJ\"] += 1\n",
    "        elif pos.startswith(\"RB\"):  # Adverbs\n",
    "            pos_counts[\"ADV\"] += 1\n",
    "    return pos_counts\n",
    "\n",
    "# Apply POS tagging to the dataset and store results in new columns\n",
    "data_balanced[\"pos_counts\"] = data_balanced[\"text\"].apply(get_pos_counts)\n",
    "\n",
    "# Convert POS dictionary to separate columns for easy analysis\n",
    "pos_df = pd.json_normalize(data_balanced[\"pos_counts\"])\n",
    "data_balanced = pd.concat([data_balanced, pos_df], axis=1)\n",
    "\n",
    "# Check the average POS counts by class\n",
    "print(\"Average POS counts by class:\")\n",
    "print(data_balanced.groupby('label')[[\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readability Scores by Class:\n",
      "         count       mean        std     min    25%    50%    75%     max\n",
      "label                                                                    \n",
      "0      17497.0  69.284005  15.872008 -446.85  63.22  70.43  77.57  103.73\n",
      "1      17497.0  55.589779  15.413828    4.58  44.34  54.02  64.88  102.41\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "\n",
    "# Function to calculate readability score\n",
    "data_balanced['readability_score'] = data_balanced['text'].apply(textstat.flesch_reading_ease)\n",
    "\n",
    "# Describe readability scores by class\n",
    "print(\"Readability Scores by Class:\")\n",
    "print(data_balanced.groupby('label')['readability_score'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Richness by Class:\n",
      "           ttr                                                              \\\n",
      "         count      mean       std       min       25%       50%       75%   \n",
      "label                                                                        \n",
      "0      17497.0  0.501099  0.084421  0.056763  0.442822  0.501377  0.559796   \n",
      "1      17497.0  0.529742  0.092877  0.127660  0.461972  0.518950  0.596078   \n",
      "\n",
      "                lexical_density                                          \\\n",
      "            max           count      mean       std       min       25%   \n",
      "label                                                                     \n",
      "0      0.774566         17497.0  0.566288  0.036557  0.427778  0.542279   \n",
      "1      1.000000         17497.0  0.592393  0.047120  0.431373  0.558824   \n",
      "\n",
      "                                     \n",
      "            50%       75%       max  \n",
      "label                                \n",
      "0      0.565543  0.589091  0.984091  \n",
      "1      0.587571  0.624434  1.050761  \n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Type-Token Ratio (TTR)\n",
    "def ttr(text):\n",
    "    words = text.split()\n",
    "    return len(set(words)) / len(words) if len(words) > 0 else 0\n",
    "\n",
    "data_balanced['ttr'] = data_balanced['text'].apply(ttr)\n",
    "\n",
    "# Lexical Density (ratio of unique content words to all words)\n",
    "def lexical_density(text):\n",
    "    blob = TextBlob(text)\n",
    "    content_words = [word for word, pos in blob.tags if pos.startswith(('NN', 'VB', 'JJ', 'RB'))]\n",
    "    return len(content_words) / len(blob.words) if len(blob.words) > 0 else 0\n",
    "\n",
    "data_balanced['lexical_density'] = data_balanced['text'].apply(lexical_density)\n",
    "\n",
    "# Summarize by class\n",
    "print(\"Vocabulary Richness by Class:\")\n",
    "print(data_balanced.groupby('label')[['ttr', 'lexical_density']].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Word Length\n",
    "import numpy as np\n",
    "\n",
    "def average_word_length(text):\n",
    "    words = text.split()\n",
    "    return np.mean([len(word) for word in words]) if len(words) > 0 else 0\n",
    "\n",
    "data_balanced['avg_word_length'] = data_balanced['text'].apply(average_word_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joshuaphilip/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Stop Word Ratio\n",
    "def stop_word_ratio(text):\n",
    "    words = text.split()\n",
    "    stop_word_count = sum(1 for word in words if word.lower() in stop_words)\n",
    "    return stop_word_count / len(words) if len(words) > 0 else 0\n",
    "\n",
    "data_balanced['stop_word_ratio'] = data_balanced['text'].apply(stop_word_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# POS Diversity\n",
    "def pos_diversity(text):\n",
    "    blob = TextBlob(text)\n",
    "    pos_tags = [pos for word, pos in blob.tags]\n",
    "    unique_pos_tags = set(pos_tags)\n",
    "    return len(unique_pos_tags) / len(pos_tags) if len(pos_tags) > 0 else 0\n",
    "\n",
    "data_balanced['pos_diversity'] = data_balanced['text'].apply(pos_diversity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Unique Word Ratio\n",
    "def unique_word_ratio(text):\n",
    "    words = text.split()\n",
    "    return len(set(words)) / len(words) if len(words) > 0 else 0\n",
    "\n",
    "data_balanced['unique_word_ratio'] = data_balanced['text'].apply(unique_word_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Word Entropy\n",
    "def word_entropy(text):\n",
    "    words = text.split()\n",
    "    word_freq = Counter(words)\n",
    "    total_words = sum(word_freq.values())\n",
    "    return -sum((freq / total_words) * np.log2(freq / total_words) for freq in word_freq.values())\n",
    "\n",
    "data_balanced['word_entropy'] = data_balanced['text'].apply(word_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Readability Indices\n",
    "# Add advanced readability metrics\n",
    "import textstat\n",
    "data_balanced['gunning_fog'] = data_balanced['text'].apply(textstat.gunning_fog)\n",
    "data_balanced['smog_index'] = data_balanced['text'].apply(textstat.smog_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Bigram and Trigram Counts\n",
    "# Add bigram and trigram frequency features\n",
    "vectorizer_bigram = CountVectorizer(ngram_range=(2, 2), stop_words='english', max_features=500)\n",
    "vectorizer_trigram = CountVectorizer(ngram_range=(3, 3), stop_words='english', max_features=500)\n",
    "\n",
    "bigram_matrix = vectorizer_bigram.fit_transform(data_balanced['text'])\n",
    "trigram_matrix = vectorizer_trigram.fit_transform(data_balanced['text'])\n",
    "\n",
    "data_balanced['bigram_count'] = bigram_matrix.sum(axis=1).A1\n",
    "data_balanced['trigram_count'] = trigram_matrix.sum(axis=1).A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_density(text):\n",
    "    sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "    if not sentences:\n",
    "        return 0\n",
    "    avg_sentence_length = np.mean([len(s.split()) for s in sentences])\n",
    "    return len(set(text.split())) / avg_sentence_length if avg_sentence_length > 0 else 0\n",
    "\n",
    "data_balanced['semantic_density'] = data_balanced['text'].apply(semantic_density)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_complexity(text):\n",
    "    sentences = text.split('.')\n",
    "    clause_counts = [sentence.count(',') + 1 for sentence in sentences]\n",
    "    return np.mean(clause_counts) if len(clause_counts) > 0 else 0\n",
    "\n",
    "data_balanced['avg_sentence_complexity'] = data_balanced['text'].apply(avg_sentence_complexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def burstiness(text):\n",
    "    sentence_lengths = [len(sentence.split()) for sentence in text.split('.')]\n",
    "    if len(sentence_lengths) == 0:\n",
    "        return 0\n",
    "    mean_length = np.mean(sentence_lengths)\n",
    "    std_dev = np.std(sentence_lengths)\n",
    "    return std_dev / mean_length if mean_length > 0 else 0\n",
    "\n",
    "data_balanced['burstiness'] = data_balanced['text'].apply(burstiness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_balanced.to_csv('processed_data_with_features.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
