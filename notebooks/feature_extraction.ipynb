{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the balanced dataset\n",
    "data_balanced = pd.read_csv('balanced_dataset.csv')\n",
    "data_balanced.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize and count common words for each class\n",
    "def get_common_words(texts, n=20):\n",
    "    words = [word.lower() for text in texts for word in text.split() if word.lower() not in stop_words]\n",
    "    return Counter(words).most_common(n)\n",
    "\n",
    "# Common words in human-written text\n",
    "human_texts = data_balanced[data_balanced['label'] == 0]['text']\n",
    "common_human_words = get_common_words(human_texts)\n",
    "print(\"Most common words in human-written text:\", common_human_words)\n",
    "\n",
    "# Common words in AI-generated text\n",
    "ai_texts = data_balanced[data_balanced['label'] == 1]['text']\n",
    "common_ai_words = get_common_words(ai_texts)\n",
    "print(\"Most common words in AI-generated text:\", common_ai_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample a subset to avoid memory issues\n",
    "sample_data = data_balanced['text'].sample(5000, random_state=42)  # Adjust if needed\n",
    "\n",
    "# Define the vectorizer with reduced n-gram range and max features\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), max_features=500, stop_words='english')  # Bigrams only, limited to 500 features\n",
    "\n",
    "# Fit and transform the sample data\n",
    "X = vectorizer.fit_transform(sample_data)\n",
    "\n",
    "# Get top n-grams with counts\n",
    "ngram_counts = X.sum(axis=0).A1  # .A1 converts sparse matrix to array\n",
    "ngram_features = vectorizer.get_feature_names()  # Use get_feature_names() for older versions\n",
    "top_ngrams = sorted(zip(ngram_features, ngram_counts), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "# Display the top 20 n-grams\n",
    "print(\"Top 20 n-grams:\", top_ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count, sentence length features were calculated in the previous notebook. Display basic stats by class.\n",
    "print(\"Word Count by Class:\")\n",
    "print(data_balanced.groupby('label')['word_count'].describe())\n",
    "\n",
    "print(\"Sentence Count by Class:\")\n",
    "print(data_balanced.groupby('label')['sentence_count'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to count punctuation\n",
    "import string\n",
    "\n",
    "def count_punctuation(text):\n",
    "    return {p: text.count(p) for p in string.punctuation}\n",
    "\n",
    "# Apply to each text entry\n",
    "data_balanced['punctuation_counts'] = data_balanced['text'].apply(count_punctuation)\n",
    "\n",
    "# Summarize average punctuation usage by class\n",
    "punctuation_summary = data_balanced.groupby('label')['punctuation_counts'].apply(lambda x: pd.DataFrame(x.tolist()).mean())\n",
    "print(\"Average punctuation usage by class:\")\n",
    "print(punctuation_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data for demonstration, using the balanced dataset\n",
    "data_balanced = pd.read_csv('balanced_dataset.csv')  # Load the balanced dataset\n",
    "\n",
    "# Define a function to extract POS counts\n",
    "def get_pos_counts(text):\n",
    "    blob = TextBlob(text)\n",
    "    pos_counts = {\"NOUN\": 0, \"VERB\": 0, \"ADJ\": 0, \"ADV\": 0}\n",
    "    for word, pos in blob.tags:\n",
    "        if pos.startswith(\"NN\"):  # Nouns\n",
    "            pos_counts[\"NOUN\"] += 1\n",
    "        elif pos.startswith(\"VB\"):  # Verbs\n",
    "            pos_counts[\"VERB\"] += 1\n",
    "        elif pos.startswith(\"JJ\"):  # Adjectives\n",
    "            pos_counts[\"ADJ\"] += 1\n",
    "        elif pos.startswith(\"RB\"):  # Adverbs\n",
    "            pos_counts[\"ADV\"] += 1\n",
    "    return pos_counts\n",
    "\n",
    "# Apply POS tagging to the dataset and store results in new columns\n",
    "data_balanced[\"pos_counts\"] = data_balanced[\"text\"].apply(get_pos_counts)\n",
    "\n",
    "# Convert POS dictionary to separate columns for easy analysis\n",
    "pos_df = pd.json_normalize(data_balanced[\"pos_counts\"])\n",
    "data_balanced = pd.concat([data_balanced, pos_df], axis=1)\n",
    "\n",
    "# Check the average POS counts by class\n",
    "print(\"Average POS counts by class:\")\n",
    "print(data_balanced.groupby('label')[[\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "# Function to calculate readability score\n",
    "data_balanced['readability_score'] = data_balanced['text'].apply(textstat.flesch_reading_ease)\n",
    "\n",
    "# Describe readability scores by class\n",
    "print(\"Readability Scores by Class:\")\n",
    "print(data_balanced.groupby('label')['readability_score'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Type-Token Ratio (TTR)\n",
    "def ttr(text):\n",
    "    words = text.split()\n",
    "    return len(set(words)) / len(words) if len(words) > 0 else 0\n",
    "\n",
    "data_balanced['ttr'] = data_balanced['text'].apply(ttr)\n",
    "\n",
    "# Lexical Density (ratio of unique content words to all words)\n",
    "def lexical_density(text):\n",
    "    blob = TextBlob(text)\n",
    "    content_words = [word for word, pos in blob.tags if pos.startswith(('NN', 'VB', 'JJ', 'RB'))]\n",
    "    return len(content_words) / len(blob.words) if len(blob.words) > 0 else 0\n",
    "\n",
    "data_balanced['lexical_density'] = data_balanced['text'].apply(lexical_density)\n",
    "\n",
    "# Summarize by class\n",
    "print(\"Vocabulary Richness by Class:\")\n",
    "print(data_balanced.groupby('label')[['ttr', 'lexical_density']].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Word Length\n",
    "import numpy as np\n",
    "\n",
    "def average_word_length(text):\n",
    "    words = text.split()\n",
    "    return np.mean([len(word) for word in words]) if len(words) > 0 else 0\n",
    "\n",
    "data_balanced['avg_word_length'] = data_balanced['text'].apply(average_word_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Stop Word Ratio\n",
    "def stop_word_ratio(text):\n",
    "    words = text.split()\n",
    "    stop_word_count = sum(1 for word in words if word.lower() in stop_words)\n",
    "    return stop_word_count / len(words) if len(words) > 0 else 0\n",
    "\n",
    "data_balanced['stop_word_ratio'] = data_balanced['text'].apply(stop_word_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# POS Diversity\n",
    "def pos_diversity(text):\n",
    "    blob = TextBlob(text)\n",
    "    pos_tags = [pos for word, pos in blob.tags]\n",
    "    unique_pos_tags = set(pos_tags)\n",
    "    return len(unique_pos_tags) / len(pos_tags) if len(pos_tags) > 0 else 0\n",
    "\n",
    "data_balanced['pos_diversity'] = data_balanced['text'].apply(pos_diversity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Unique Word Ratio\n",
    "def unique_word_ratio(text):\n",
    "    words = text.split()\n",
    "    return len(set(words)) / len(words) if len(words) > 0 else 0\n",
    "\n",
    "data_balanced['unique_word_ratio'] = data_balanced['text'].apply(unique_word_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Word Entropy\n",
    "def word_entropy(text):\n",
    "    words = text.split()\n",
    "    word_freq = Counter(words)\n",
    "    total_words = sum(word_freq.values())\n",
    "    return -sum((freq / total_words) * np.log2(freq / total_words) for freq in word_freq.values())\n",
    "\n",
    "data_balanced['word_entropy'] = data_balanced['text'].apply(word_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Readability Indices\n",
    "# Add advanced readability metrics\n",
    "import textstat\n",
    "data_balanced['gunning_fog'] = data_balanced['text'].apply(textstat.gunning_fog)\n",
    "data_balanced['smog_index'] = data_balanced['text'].apply(textstat.smog_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Bigram and Trigram Counts\n",
    "# Add bigram and trigram frequency features\n",
    "vectorizer_bigram = CountVectorizer(ngram_range=(2, 2), stop_words='english', max_features=500)\n",
    "vectorizer_trigram = CountVectorizer(ngram_range=(3, 3), stop_words='english', max_features=500)\n",
    "\n",
    "bigram_matrix = vectorizer_bigram.fit_transform(data_balanced['text'])\n",
    "trigram_matrix = vectorizer_trigram.fit_transform(data_balanced['text'])\n",
    "\n",
    "data_balanced['bigram_count'] = bigram_matrix.sum(axis=1).A1\n",
    "data_balanced['trigram_count'] = trigram_matrix.sum(axis=1).A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_density(text):\n",
    "    sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "    if not sentences:\n",
    "        return 0\n",
    "    avg_sentence_length = np.mean([len(s.split()) for s in sentences])\n",
    "    return len(set(text.split())) / avg_sentence_length if avg_sentence_length > 0 else 0\n",
    "\n",
    "data_balanced['semantic_density'] = data_balanced['text'].apply(semantic_density)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_complexity(text):\n",
    "    sentences = text.split('.')\n",
    "    clause_counts = [sentence.count(',') + 1 for sentence in sentences]\n",
    "    return np.mean(clause_counts) if len(clause_counts) > 0 else 0\n",
    "\n",
    "data_balanced['avg_sentence_complexity'] = data_balanced['text'].apply(avg_sentence_complexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def burstiness(text):\n",
    "    sentence_lengths = [len(sentence.split()) for sentence in text.split('.')]\n",
    "    if len(sentence_lengths) == 0:\n",
    "        return 0\n",
    "    mean_length = np.mean(sentence_lengths)\n",
    "    std_dev = np.std(sentence_lengths)\n",
    "    return std_dev / mean_length if mean_length > 0 else 0\n",
    "\n",
    "data_balanced['burstiness'] = data_balanced['text'].apply(burstiness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_balanced.to_csv('processed_data_with_features.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
