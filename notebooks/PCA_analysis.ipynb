{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# Load the processed dataset with all features\n",
    "data_with_features = pd.read_csv('processed_data_with_features.csv')\n",
    "\n",
    "# Define the list of features to use for PCA\n",
    "features_to_use = [\n",
    "    'word_count', 'char_count', 'sentence_count',\n",
    "    'NOUN', 'VERB', 'ADJ', 'ADV',\n",
    "    'readability_score', 'ttr', 'lexical_density',\n",
    "    'avg_word_length', 'stop_word_ratio', 'pos_diversity',\n",
    "    'unique_word_ratio', 'word_entropy', 'gunning_fog', 'smog_index',\n",
    "    'bigram_count', 'trigram_count', 'semantic_density', 'avg_sentence_complexity', 'burstiness'\n",
    "\n",
    "]\n",
    "# Separate features (X) and target labels (y)\n",
    "X = data_with_features[features_to_use]\n",
    "y = data_with_features['label']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA without specifying components to get all components\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA Explained Variance')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Print the number of components to retain at 95% explained variance\n",
    "optimal_components = np.argmax(cumulative_variance >= 0.95) + 1  # Add 1 because index starts at 0\n",
    "print(f\"Optimal number of components to retain 95% explained variance: {optimal_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the principal component loadings\n",
    "loadings = pca.components_\n",
    "feature_names = X.columns\n",
    "loading_matrix = pd.DataFrame(loadings.T, columns=[f\"PC{i+1}\" for i in range(loadings.shape[0])], index=feature_names)\n",
    "\n",
    "# Calculate feature importance as the sum of absolute contributions to the retained components\n",
    "feature_importance = loading_matrix.iloc[:, :optimal_components].abs().sum(axis=1).sort_values(ascending=False)\n",
    "\n",
    "# Print feature importance\n",
    "print(\"\\nFeature importance based on contribution to top components:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Identify features with minimal contributions\n",
    "low_contributing_features = feature_importance[feature_importance < feature_importance.mean()].index.tolist()\n",
    "print(f\"\\nFeatures with minimal contributions to top {optimal_components} components: {low_contributing_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update feature list by removing low-contributing features\n",
    "updated_features = [feature for feature in features_to_use if feature not in low_contributing_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the updated feature list\n",
    "X_updated = data_with_features[updated_features]\n",
    "\n",
    "# Standardize the updated features\n",
    "X_scaled_updated = scaler.fit_transform(X_updated)\n",
    "\n",
    "# Apply PCA with optimal components\n",
    "pca_updated = PCA(n_components=optimal_components)\n",
    "X_pca_updated = pca_updated.fit_transform(X_scaled_updated)\n",
    "\n",
    "# Print updated explained variance\n",
    "explained_variance_updated = pca_updated.explained_variance_ratio_\n",
    "print(f\"Updated explained variance by principal components: {explained_variance_updated}\")\n",
    "print(f\"Total updated explained variance: {np.sum(explained_variance_updated)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the PCA-transformed data\n",
    "X_pca_df = pd.DataFrame(X_pca_updated, columns=[f\"PC{i+1}\" for i in range(X_pca_updated.shape[1])])\n",
    "X_pca_df['label'] = y\n",
    "X_pca_df.to_csv('pca_transformed_data.csv', index=False)\n",
    "print(\"PCA-transformed data saved as 'pca_transformed_data.csv'.\")\n",
    "\n",
    "# Save the PCA model and scaler\n",
    "joblib.dump(pca_updated, 'updated_pca_model.pkl')\n",
    "joblib.dump(scaler, 'updated_scaler.pkl')\n",
    "print(\"PCA model and scaler saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load the processed dataset with all features\n",
    "data_with_features = pd.read_csv('processed_data_with_features.csv')\n",
    "\n",
    "# Define the list of features to use for PCA\n",
    "features_to_use = [\n",
    "    'word_count', 'char_count', 'sentence_count',\n",
    "    'NOUN', 'VERB', 'ADJ', 'ADV',\n",
    "    'readability_score', 'ttr', 'lexical_density',\n",
    "    'avg_word_length', 'stop_word_ratio', 'pos_diversity',\n",
    "    'unique_word_ratio', 'word_entropy', 'gunning_fog', 'smog_index',\n",
    "    'bigram_count', 'trigram_count'\n",
    "]\n",
    "\n",
    "# Separate features (X) and target labels (y)\n",
    "X = data_with_features[features_to_use]\n",
    "y = data_with_features['label']\n",
    "\n",
    "# Step 2: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Apply PCA to reduce to the top 2 principal components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca_top2 = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Explained variance by the top 2 components\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained variance by top 2 principal components: {explained_variance}\")\n",
    "print(f\"Total explained variance (top 2 components): {np.sum(explained_variance):.4f}\")\n",
    "\n",
    "# Step 4: Analyze Feature Contributions to the Top 2 Components\n",
    "# Get the principal component loadings\n",
    "loadings = pca.components_\n",
    "feature_names = X.columns\n",
    "pc_loadings = pd.DataFrame(loadings.T, columns=['PC1', 'PC2'], index=feature_names)\n",
    "\n",
    "# Identify the top contributing features for PC1 and PC2\n",
    "top_features_pc1 = pc_loadings['PC1'].abs().sort_values(ascending=False).head(5)\n",
    "top_features_pc2 = pc_loadings['PC2'].abs().sort_values(ascending=False).head(5)\n",
    "\n",
    "print(\"\\nTop contributing features to PC1:\")\n",
    "print(top_features_pc1)\n",
    "\n",
    "print(\"\\nTop contributing features to PC2:\")\n",
    "print(top_features_pc2)\n",
    "\n",
    "# Step 5: Save PCA-transformed data (top 2 components)\n",
    "X_pca_top2_df = pd.DataFrame(X_pca_top2, columns=['PC1', 'PC2'])\n",
    "X_pca_top2_df['label'] = y\n",
    "X_pca_top2_df.to_csv('pca_top2_transformed_data.csv', index=False)\n",
    "print(\"PCA-transformed data (top 2 components) saved as 'pca_top2_transformed_data.csv'.\")\n",
    "\n",
    "# Step 6: Save the PCA model and scaler\n",
    "joblib.dump(pca, 'pca_top2_model.pkl')\n",
    "joblib.dump(scaler, 'scaler_top2.pkl')\n",
    "print(\"PCA model and scaler for top 2 components saved.\")\n",
    "\n",
    "# Step 7: Visualize the top 2 PCA components\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(X_pca_top2[:, 0], X_pca_top2[:, 1], c=y, cmap='coolwarm', alpha=0.6)\n",
    "plt.colorbar(scatter, label='Label')\n",
    "plt.title('Top 2 PCA Components')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid()\n",
    "plt.savefig('pca_top2_plot.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
